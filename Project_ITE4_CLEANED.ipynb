{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pqwoRCAZCYKq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "282b77dea13041949a125ae42aaa4f9f",
      "21207fe91d8148769cd33b23dcbf337f",
      "13e04b277f3f406babfb1a83e6663440",
      "3f78eff4be6a46fcad6eb682dcb9d8bf",
      "4571d0276b2e40208dd997cee54a648c",
      "3982eb6cb72543ebb2c91d1d99364978",
      "256ce14c008840399d5bd74d169aa197",
      "50cfd64474c844d19a4254df5e356b7a",
      "26a99b2c6bb84363b6f8b8c09ea6b812",
      "971a19f9d27346199c7024abb0264d9e",
      "7b52288c3d124d9d936d8c904d08ccbf",
      "bc1681cd3e054c1d9b23a0b2823d27d5",
      "63437d094e094a8a88614c109454cdcb",
      "eb917f7599f44da597842a1f863a15e6",
      "e77afd59d7384ed29162832e2cc6a38b",
      "5da400a826684795a365a9f7494bdf26",
      "0ef109681cfa48579f83128ec76ed184",
      "05c89188d92a4c6d9fb6cd524a286035",
      "308cee9c7ede41899fa5c5f42106dd52",
      "98eaf9e3ddf34366ac2b400d08f7851d",
      "89ea1a35fe53464b933b2a5d991a0828",
      "d41f15c4120544ccbd4f26229075d547",
      "f356b52e13db44bcb3a94d9e27d6f38e",
      "19e5e5a688924dc0ac6965660fd5d886",
      "6034710bdcb14681b6dda84a6d8a6a2e",
      "4552675b24894200b55fa57e13c7d0c6",
      "848e87fefa964be28236754487dcc889",
      "437b01fa69e2489095344f407eba0cc0",
      "f1f5439eae274e97ace207a91ec9f7f4",
      "e05242f9d8b14489b483e503c26c39fc",
      "553d66885c6a483cb94cc12a1cede46d",
      "4aeb7b2d92784661a1e5112e4f014e0c",
      "2d0d54de04b341dca22c30fea9877d2f",
      "f505aeabdb1d4da99273716997a0a850",
      "55bf9d4756624b9ba0a6fe2bf7879102",
      "9f77825396934c7fbcdb59941ebba1ea",
      "d25ccebc00b7469fa877048e17185f22",
      "a4f591ac6a87492699a767a4ca8c3821",
      "4d3582c8c8734879b6b18cac5b2371e9",
      "06f21369062c43a9aca0dfa2e0c17ed6",
      "10587996c8d2430a8b6bda9467ddae84",
      "1cfb893eba3e4154b9118351e73bbb4e",
      "d13e923fc4884e3fbc632d50a6439315",
      "dc46187d14654c42bac69f2f2b92c19d",
      "beb8851a059f4d7497e0a1c550d25e9d",
      "6fb012086e1141a3b5ef6af659392aef",
      "2334fc02e95a442da48a82ea23b1a11c",
      "b5ea4fd6a93d4428b5f5f6816f3f408a",
      "6d66606d62e44e849a95a3402f75c744",
      "2a52379d6cc14d609d8c06c53f00018d",
      "11abd1ac4a9e499cbf2de60202ce89e0",
      "ed6d0f84ba2e421f9225cf352a02ca70",
      "aa13a6f732ed4b4f86ea059c08d0dbd8",
      "1f7a20a3a8a047699ccd3184cf3cb753",
      "c1e1d5bed9804179a10e800669bf2726",
      "b74bca641276400d8fb557ed0a85d34d",
      "562c239a79f643cf902198b109e59045",
      "cf5d9feb5e6f4af09bfbb7900a9bd5cb",
      "c9dde04eafaa427fa174fc45053599e4",
      "420daabc607d498e9dec4de98ae52e63",
      "0f788e4682c747a1b8cb610de938e775",
      "08057217dc48405e9acb598e362eecf4",
      "e5a9e18d1e7142e5b758a7a8cc9977f1",
      "98f9c83b40d849b6a7bf85551b4aafaa",
      "4319d4c3e117483186f83aa25d84da50",
      "99971f88178741d3a40e2d0e9e70be68",
      "55578b12dc8f40acb8ebffbb10ee18ef",
      "59df8d8b13644f7d862c741ab9e936ef",
      "8153ab323e224ddf855d5f29e0d75348",
      "97ea9103e8544ff1ada21cf765f7d938",
      "72712b9cb2144260869260f22de15c86",
      "2a0b918764d64943ac48e5704d20cb75",
      "f355b59bc63d434594121c6effd9c966",
      "71a85dcaa33041f3b26b264981fb5a01",
      "5b2b8e588506426093eefdaf50322821",
      "76ca4c80ba43454c8d6aa90924923dbd",
      "61aea12c55ce4f2b9f1315c2c9a3cd17",
      "b6e67719818b430ab5452fff20e80f0a",
      "135040eafff648d28e47baea14d9aad4",
      "274844b9b4fa43cbacf16f115429ac01",
      "75c71ca5a193497e8333bb7f24f3fbc6",
      "8514a9bc041344e4911688be144fcdbd",
      "abe0a7a0ebcb469580ac178f0d0ecdc8",
      "ba64a1eb9e0e4c43a3cb09875c72e715",
      "80efcfba0e0f4dde8c11e92d41e89983",
      "466bae77eaa24be38ac72eb5aabb261c",
      "ac6c8671557a4bf3a493ee59a9c63884",
      "e051aa5f50ef494da4120087bfdd679c"
     ]
    },
    "outputId": "93208477-4565-4efd-94c4-bfc03df2c11b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "CLICKBAIT DETECTION - BEST HYPERPARAMETERS\n",
      "============================================================\n",
      "Learning Rate: 3e-05\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.05\n",
      "Number of Epochs: 3\n",
      "============================================================\n",
      "\n",
      "\u2713 Using GPU: Tesla T4\n",
      "\n",
      "--- Loading and Preprocessing Data ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-c43f4653-34f3-4f48-82d3-410e5f648690\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-c43f4653-34f3-4f48-82d3-410e5f648690\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving archive (5).zip to archive (5).zip\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "282b77dea13041949a125ae42aaa4f9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset loaded: 32000 total samples\n",
      "Training samples: 2000\n",
      "Evaluation samples: 500\n",
      "\n",
      "--- Tokenizing Data ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc1681cd3e054c1d9b23a0b2823d27d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f356b52e13db44bcb3a94d9e27d6f38e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f505aeabdb1d4da99273716997a0a850"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "beb8851a059f4d7497e0a1c550d25e9d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b74bca641276400d8fb557ed0a85d34d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55578b12dc8f40acb8ebffbb10ee18ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Tokenization complete\n",
      "\n",
      "--- Loading Pre-trained Model ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6e67719818b430ab5452fff20e80f0a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Model loaded: distilbert-base-multilingual-cased\n",
      "  Number of parameters: 135,326,210\n",
      "\n",
      "--- Configuring Training with Best Hyperparameters ---\n",
      "\u2713 Training configuration complete\n",
      "\n",
      "============================================================\n",
      "TRAINING MODEL WITH BEST HYPERPARAMETERS\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.058024</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.972868</td>\n",
       "      <td>0.996032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.031453</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990060</td>\n",
       "      <td>0.992032</td>\n",
       "      <td>0.988095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.034366</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990138</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.996032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2713 Training complete!\n",
      "\n",
      "============================================================\n",
      "FINAL EVALUATION ON TEST SET\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udcca FINAL TEST SET RESULTS:\n",
      "  Accuracy:  0.9900\n",
      "  F1 Score:  0.9901\n",
      "  Precision: 0.9843\n",
      "  Recall:    0.9960\n",
      "\n",
      "--- Generating Detailed Classification Report ---\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Clickbait       1.00      0.98      0.99       248\n",
      "    Clickbait       0.98      1.00      0.99       252\n",
      "\n",
      "     accuracy                           0.99       500\n",
      "    macro avg       0.99      0.99      0.99       500\n",
      " weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[244   4]\n",
      " [  1 251]]\n",
      "\n",
      "True Negatives:  244\n",
      "False Positives: 4\n",
      "False Negatives: 1\n",
      "True Positives:  251\n",
      "\n",
      "--- Saving Results ---\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "mount failed",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1313120858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;31m# Prepare results dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_score, recall_score\n",
    "from google.colab import files, drive\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# BEST HYPERPARAMETERS (Manually Applied)\n",
    "# ============================================================\n",
    "# Based on your experimentation results:\n",
    "# learning_rate: 3e-05\n",
    "# batch_size: 16\n",
    "# weight_decay: 0.05\n",
    "# num_train_epochs: 3 (from Run 3)\n",
    "# Expected Performance: F1 ~0.9971, Accuracy ~0.9934\n",
    "# ============================================================\n",
    "\n",
    "BEST_LEARNING_RATE = 3e-05\n",
    "BEST_BATCH_SIZE = 16\n",
    "BEST_WEIGHT_DECAY = 0.05\n",
    "BEST_NUM_EPOCHS = 3\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLICKBAIT DETECTION - BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Learning Rate: {BEST_LEARNING_RATE}\")\n",
    "print(f\"Batch Size: {BEST_BATCH_SIZE}\")\n",
    "print(f\"Weight Decay: {BEST_WEIGHT_DECAY}\")\n",
    "print(f\"Number of Epochs: {BEST_NUM_EPOCHS}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"\\n\u2713 Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n\u26a0 GPU not available, using CPU.\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# ============================================================\n",
    "print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "# Upload the CSV from your computer\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"csv\", data_files=list(uploaded.keys()))\n",
    "print(f\"Dataset loaded: {len(dataset['train'])} total samples\")\n",
    "\n",
    "# Split into train/test (80/20 split)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Use subset for faster training (adjust as needed)\n",
    "train_data = dataset[\"train\"].select(range(min(2000, len(dataset[\"train\"]))))\n",
    "eval_data = dataset[\"test\"].select(range(min(500, len(dataset[\"test\"]))))\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Evaluation samples: {len(eval_data)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. TOKENIZATION\n",
    "# ============================================================\n",
    "print(\"\\n--- Tokenizing Data ---\")\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize headlines for model input\"\"\"\n",
    "    return tokenizer(examples[\"headline\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_data.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "tokenized_train.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'clickbait'])\n",
    "tokenized_eval.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'clickbait'])\n",
    "\n",
    "# Rename 'clickbait' column to 'label' (expected by Trainer)\n",
    "tokenized_train = tokenized_train.rename_column(\"clickbait\", \"label\")\n",
    "tokenized_eval = tokenized_eval.rename_column(\"clickbait\", \"label\")\n",
    "\n",
    "print(\"\u2713 Tokenization complete\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. MODEL INITIALIZATION\n",
    "# ============================================================\n",
    "print(\"\\n--- Loading Pre-trained Model ---\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"\u2713 Model loaded: {MODEL_NAME}\")\n",
    "print(f\"  Number of parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. DEFINE METRICS\n",
    "# ============================================================\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Calculate accuracy, F1, precision, and recall\"\"\"\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"binary\")\n",
    "    precision = precision_score(labels, preds, average=\"binary\")\n",
    "    recall = recall_score(labels, preds, average=\"binary\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 5. TRAINING CONFIGURATION (BEST HYPERPARAMETERS)\n",
    "# ============================================================\n",
    "print(\"\\n--- Configuring Training with Best Hyperparameters ---\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./clickbait_model_best\",\n",
    "\n",
    "    # BEST HYPERPARAMETERS\n",
    "    learning_rate=BEST_LEARNING_RATE,\n",
    "    per_device_train_batch_size=BEST_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BEST_BATCH_SIZE,\n",
    "    weight_decay=BEST_WEIGHT_DECAY,\n",
    "    num_train_epochs=BEST_NUM_EPOCHS,\n",
    "\n",
    "    # Evaluation and saving strategy\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "\n",
    "    # Logging\n",
    "    logging_dir=\"./logs_best\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    # Optimization\n",
    "    warmup_steps=100,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "\n",
    "    # Disable external logging\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training configuration complete\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. INITIALIZE TRAINER\n",
    "# ============================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 7. TRAIN THE MODEL\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\u2713 Training complete!\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. FINAL EVALUATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\ud83d\udcca FINAL TEST SET RESULTS:\")\n",
    "print(f\"  Accuracy:  {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1 Score:  {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"  Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"  Recall:    {eval_results['eval_recall']:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. DETAILED CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "print(\"\\n--- Generating Detailed Classification Report ---\")\n",
    "\n",
    "predictions = trainer.predict(tokenized_eval)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Not Clickbait\", \"Clickbait\"]))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0][0]}\")\n",
    "print(f\"False Positives: {cm[0][1]}\")\n",
    "print(f\"False Negatives: {cm[1][0]}\")\n",
    "print(f\"True Positives:  {cm[1][1]}\")\n",
    "\n",
    "# ============================================================\n",
    "# 10. SAVE RESULTS TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "print(\"\\n--- Saving Results ---\")\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Prepare results dictionary\n",
    "results_dict = {\n",
    "    \"Hyperparameter\": [\"Learning Rate\", \"Batch Size\", \"Weight Decay\", \"Num Epochs\"],\n",
    "    \"Value\": [BEST_LEARNING_RATE, BEST_BATCH_SIZE, BEST_WEIGHT_DECAY, BEST_NUM_EPOCHS]\n",
    "}\n",
    "\n",
    "metrics_dict = {\n",
    "    \"Metric\": [\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"],\n",
    "    \"Score\": [\n",
    "        eval_results['eval_accuracy'],\n",
    "        eval_results['eval_f1'],\n",
    "        eval_results['eval_precision'],\n",
    "        eval_results['eval_recall']\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "with pd.ExcelWriter('/content/drive/MyDrive/clickbait_best_results.xlsx') as writer:\n",
    "    results_df.to_excel(writer, sheet_name='Hyperparameters', index=False)\n",
    "    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "\n",
    "print(\"\u2713 Results saved to Google Drive: clickbait_best_results.xlsx\")\n",
    "\n",
    "# ============================================================\n",
    "# 11. TEST ON NEW HEADLINES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING ON NEW HEADLINES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create prediction pipeline\n",
    "clickbait_detector = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test headlines\n",
    "new_headlines = [\n",
    "    \"MMDA Launches New Traffic Scheme to Ease Congestion Along EDSA\",\n",
    "    \"DepEd Confirms Opening of Classes Will Proceed as Scheduled\",\n",
    "    \"DOH Reports Steady Decline in Dengue Cases Nationwide\",\n",
    "    \"You Won't Believe What This Filipino Celebrity Did After Winning the Lottery!\",\n",
    "    \"This One Trick Can Help You Save Thousands on Your Meralco Bill!\",\n",
    "    \"Students in Manila Tried This Study Method\u2014The Results Will Shock You!\",\n",
    "    \"Comelec Prepares for 2025 Elections With Improved Voter Registration System\",\n",
    "    \"PH Economy Grows by 5.8% in Third Quarter, Says PSA\",\n",
    "    \"A Mayor's Secret Finally Revealed\u2014The Whole Town Is Talking About It!\",\n",
    "    \"Here's Why Everyone Is Rushing to Try This New Food Trend in Quezon City!\",\n",
    "]\n",
    "\n",
    "results = clickbait_detector(new_headlines)\n",
    "\n",
    "print(\"\\n\ud83d\udcf0 PREDICTIONS ON NEW HEADLINES:\\n\")\n",
    "for text, result in zip(new_headlines, results):\n",
    "    label = result[\"label\"]\n",
    "    prediction = \"\ud83c\udfa3 Clickbait\" if label in [\"LABEL_1\", \"1\"] else \"\u2713 Not Clickbait\"\n",
    "    confidence = result['score']\n",
    "    print(f\"{prediction} ({confidence:.2%})\")\n",
    "    print(f\"   {text}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 12. SAVE THE MODEL\n",
    "# ============================================================\n",
    "print(\"\\n--- Saving Trained Model ---\")\n",
    "\n",
    "model.save_pretrained(\"./clickbait_model_final\")\n",
    "tokenizer.save_pretrained(\"./clickbait_model_final\")\n",
    "\n",
    "print(\"\u2713 Model saved to: ./clickbait_model_final\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ]
}