{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLPDArsYpq5lorycWptg6R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVTYI12mwJx5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1762576798961,
          "user_tz": -480,
          "elapsed": 19487,
          "user": {
            "displayName": "Psalmiel Joshua Jose",
            "userId": "09868482139927997770"
          }
        },
        "outputId": "a17e3c27-959d-4401-d985-7d61d711dcca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pyarrow, colorlog, optuna, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorlog-6.10.1 datasets-4.4.1 optuna-4.5.0 pyarrow-22.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers datasets accelerate optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback # <--- NEW: Imported for early stopping\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Global Configuration ---\n",
        "set_seed(42)\n",
        "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
        "data_path = \"/content/drive/MyDrive/ITC508_data/clickbait_data.csv\"\n",
        "output_dir = \"./grid_search_results\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# --- Load Dataset & Split ---\n",
        "df = pd.read_csv(data_path)\n",
        "df = df.rename(columns={\"headline\": \"text\", \"clickbait\": \"label\"})\n",
        "print(f\"Original Data Distribution:\\n{df['label'].value_counts()}\")\n",
        "\n",
        "# Split into Train, Validation, and Test (64%, 16%, 20%)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df) # <--- NEW: Test set for final evaluation\n",
        "\n",
        "# --- Tokenization ---\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[\"__index_level_0__\", \"text\"])\n",
        "val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=[\"__index_level_0__\", \"text\"])\n",
        "test_dataset = test_dataset.map(tokenize, batched=True, remove_columns=[\"__index_level_0__\", \"text\"]) # Tokenize test set\n",
        "\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# --- Metrics ---\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
        "        \"f1\": f1_score(p.label_ids, preds),\n",
        "        \"precision\": precision_score(p.label_ids, preds),\n",
        "        \"recall\": recall_score(p.label_ids, preds)\n",
        "    }\n",
        "\n",
        "# --- Model Initialization ---\n",
        "def model_init():\n",
        "    # Model is initialized on CPU and moved to device by the Trainer\n",
        "    return DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
        "\n",
        "# --- Training Arguments (Base Configuration) ---\n",
        "base_kwargs = {\n",
        "    \"output_dir\": output_dir,\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"load_best_model_at_end\": True, # Crucial for Early Stopping\n",
        "    \"metric_for_best_model\": \"f1\", # Monitor F1, as accuracy is too high\n",
        "    \"greater_is_better\": True,\n",
        "    \"fp16\": torch.cuda.is_available(),\n",
        "    \"report_to\": \"none\",\n",
        "    \"num_train_epochs\": 10, # Increased epochs with Early Stopping\n",
        "}\n",
        "\n",
        "# Adjust for evaluation strategy naming based on library version\n",
        "if hasattr(TrainingArguments, \"evaluation_strategy\"):\n",
        "    base_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
        "else:\n",
        "    base_kwargs[\"eval_strategy\"] = \"epoch\"\n",
        "\n",
        "# --- Hyperparameter Grid (Refined for Regularization) ---\n",
        "# Focus on lower learning rates and higher weight decay to combat overfitting\n",
        "search_space = [\n",
        "    {\"learning_rate\": 5e-5, \"batch_size\": 8, \"weight_decay\": 0.0},\n",
        "    {\"learning_rate\": 3e-5, \"batch_size\": 16, \"weight_decay\": 0.05},\n",
        "    {\"learning_rate\": 1e-5, \"batch_size\": 8, \"weight_decay\": 0.05},\n",
        "    {\"learning_rate\": 1e-5, \"batch_size\": 16, \"weight_decay\": 0.1}, # Strongest Regularization combo\n",
        "]\n",
        "\n",
        "best_score = 0\n",
        "best_params = None\n",
        "best_model_path = None\n",
        "run_index = 0\n",
        "\n",
        "# --- Grid Search Loop ---\n",
        "for params in search_space:\n",
        "    run_index += 1\n",
        "    print(f\"\\n========================================================\")\n",
        "    print(f\"\ud83d\udd0d Run {run_index}/{len(search_space)}: Training with: {params}\")\n",
        "    print(f\"========================================================\")\n",
        "\n",
        "    # Update base args with specific grid parameters\n",
        "    training_args = TrainingArguments(\n",
        "        **base_kwargs,\n",
        "        per_device_train_batch_size=params[\"batch_size\"],\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "        # Save checkpoints for this run to a unique folder\n",
        "        output_dir=f\"{output_dir}/run_{run_index}\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        # --- NEW: Add Early Stopping Callback (Patience=2 means stop if no improvement for 2 epochs) ---\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    print(metrics)\n",
        "\n",
        "    # Track best model and save path\n",
        "    if metrics[\"eval_f1\"] > best_score:\n",
        "        best_score = metrics[\"eval_f1\"]\n",
        "        best_params = params\n",
        "        # The best model checkpoint is saved by load_best_model_at_end=True\n",
        "        # We save the *actual* best model from the current run\n",
        "        trainer.save_model(f\"{output_dir}/best_model_final\")\n",
        "        best_model_path = f\"{output_dir}/best_model_final\"\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"\ud83c\udfc6 BEST HYPERPARAMETERS FOUND:\")\n",
        "print(best_params)\n",
        "print(f\"Best Validation F1: {best_score:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- Final Evaluation on Test Set ---\n",
        "if best_model_path:\n",
        "    print(\"\\n\ud83d\udcdd Evaluating Best Model on Held-out Test Set...\")\n",
        "\n",
        "    # Load the best model found during the grid search\n",
        "    final_model = DistilBertForSequenceClassification.from_pretrained(best_model_path).to(device)\n",
        "\n",
        "    # Create a new trainer for evaluation only\n",
        "    final_trainer = Trainer(\n",
        "        model=final_model,\n",
        "        args=TrainingArguments(output_dir=\"./test_eval\", report_to=\"none\"), # Simple args for evaluation\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    test_metrics = final_trainer.evaluate(test_dataset)\n",
        "\n",
        "    print(\"\\n\\n\" + \"*\"*50)\n",
        "    print(\"\u2b50 FINAL TEST SET RESULTS \u2b50\")\n",
        "    print(f\"Best Params: {best_params}\")\n",
        "    print(f\"Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n",
        "    print(f\"F1 Score: {test_metrics['eval_f1']:.4f}\")\n",
        "    print(f\"Precision: {test_metrics['eval_precision']:.4f}\")\n",
        "    print(f\"Recall: {test_metrics['eval_recall']:.4f}\")\n",
        "    print(\"*\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6482d614af5f437687eaa47680e0c3f3",
            "9c7d07508f7b4c96a1e41313fd0443a9",
            "edef43ce207346199d15b4840198f76e",
            "47442c67e0bc4868abde171d07aed17e",
            "3ca90d29fbbe47b887e291eb220c3854",
            "2b83ab304fd94fe8bb4075bb7619ac1f",
            "3e360094affd4671a61486cf63da0dd4",
            "2f407dd8e00c41a5b07d226c3d2a33da",
            "44362a8cbee34479bbda2537aa94d0cd",
            "7682e38bc9064209b80f44a7279489d3",
            "c042cc591aa04283b1c568a16263f640",
            "53c9e7d81c864309aeebe32119e8b2f2",
            "2aa3e9bd22e54578b6f748cc6088862c",
            "f619b4d1fc824d609fd2d106d85af4f1",
            "b810a604e4bd4601b364db63bb912337",
            "df28beb77f4f4d83b6c96657870b9134",
            "abcd670936054240ba644a5a980e6291",
            "7093086f0c204972873fd7987335eb27",
            "4b7c02ed6e6144b3966a1c965d0bed4b",
            "7a80c41e42884341bcb7e6ac3ab8dafc",
            "b7317e0d3e4e48e684f6d433aaef608d",
            "fab70831a1484d498a93caa8ab4bb728",
            "969b2f058abf484289b4295085741bc4",
            "6b71a0b970f84ebeaf08d6a472e7fa7b",
            "6bb3b9471d064428b2b3967e788214dc",
            "0241e693486b48d488b20bac71bc8334",
            "3207b55d681049be96d9c074f635dced",
            "f57e6dd7533d4f24be79240bb67e4612",
            "c3fb50aff6e640aeae8a4ea7d6684459",
            "cdfb347ada814ac2a3b1c650a9930867",
            "b477b0fd401c44e388e4523290e4c99e",
            "966ccffb04674363a8a6dbf07f77911d",
            "32328f8ef4a947afbd1de6fdb4cb9bb9"
          ]
        },
        "id": "XyC02gTpwWrv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1762582076162,
          "user_tz": -480,
          "elapsed": 4047223,
          "user": {
            "displayName": "Psalmiel Joshua Jose",
            "userId": "09868482139927997770"
          }
        },
        "outputId": "a51283f6-0890-4112-8501-ddd48abce732"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Original Data Distribution:\n",
            "label\n",
            "0    16001\n",
            "1    15999\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20480 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6482d614af5f437687eaa47680e0c3f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5120 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53c9e7d81c864309aeebe32119e8b2f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "969b2f058abf484289b4295085741bc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================================\n",
            "\ud83d\udd0d Run 1/4: Training with: {'learning_rate': 5e-05, 'batch_size': 8, 'weight_decay': 0.0}\n",
            "========================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3423641199.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17920' max='25600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17920/25600 23:15 < 09:58, 12.84 it/s, Epoch 7/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.039208</td>\n",
              "      <td>0.993164</td>\n",
              "      <td>0.993187</td>\n",
              "      <td>0.989911</td>\n",
              "      <td>0.996484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.021100</td>\n",
              "      <td>0.042953</td>\n",
              "      <td>0.993359</td>\n",
              "      <td>0.993365</td>\n",
              "      <td>0.992590</td>\n",
              "      <td>0.994141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.034579</td>\n",
              "      <td>0.994336</td>\n",
              "      <td>0.994319</td>\n",
              "      <td>0.997250</td>\n",
              "      <td>0.991406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.008100</td>\n",
              "      <td>0.040918</td>\n",
              "      <td>0.995313</td>\n",
              "      <td>0.995322</td>\n",
              "      <td>0.993385</td>\n",
              "      <td>0.997266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>0.037855</td>\n",
              "      <td>0.995508</td>\n",
              "      <td>0.995512</td>\n",
              "      <td>0.994542</td>\n",
              "      <td>0.996484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.035960</td>\n",
              "      <td>0.994336</td>\n",
              "      <td>0.994341</td>\n",
              "      <td>0.993372</td>\n",
              "      <td>0.995313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.048419</td>\n",
              "      <td>0.994727</td>\n",
              "      <td>0.994717</td>\n",
              "      <td>0.996472</td>\n",
              "      <td>0.992969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [640/640 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.03785516694188118, 'eval_accuracy': 0.9955078125, 'eval_f1': 0.9955121951219512, 'eval_precision': 0.9945419103313841, 'eval_recall': 0.996484375, 'eval_runtime': 6.4177, 'eval_samples_per_second': 797.794, 'eval_steps_per_second': 99.724, 'epoch': 7.0}\n",
            "\n",
            "========================================================\n",
            "\ud83d\udd0d Run 2/4: Training with: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.05}\n",
            "========================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3423641199.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5120' max='12800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5120/12800 11:10 < 16:46, 7.63 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.015631</td>\n",
              "      <td>0.995703</td>\n",
              "      <td>0.995712</td>\n",
              "      <td>0.993774</td>\n",
              "      <td>0.997656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.009900</td>\n",
              "      <td>0.017002</td>\n",
              "      <td>0.997070</td>\n",
              "      <td>0.997071</td>\n",
              "      <td>0.996876</td>\n",
              "      <td>0.997266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.006800</td>\n",
              "      <td>0.029113</td>\n",
              "      <td>0.995508</td>\n",
              "      <td>0.995496</td>\n",
              "      <td>0.998037</td>\n",
              "      <td>0.992969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.026627</td>\n",
              "      <td>0.995898</td>\n",
              "      <td>0.995896</td>\n",
              "      <td>0.996480</td>\n",
              "      <td>0.995313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [640/640 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.017002161592245102, 'eval_accuracy': 0.9970703125, 'eval_f1': 0.997070884592853, 'eval_precision': 0.996876220226474, 'eval_recall': 0.997265625, 'eval_runtime': 6.3958, 'eval_samples_per_second': 800.522, 'eval_steps_per_second': 100.065, 'epoch': 4.0}\n",
            "\n",
            "========================================================\n",
            "\ud83d\udd0d Run 3/4: Training with: {'learning_rate': 1e-05, 'batch_size': 8, 'weight_decay': 0.05}\n",
            "========================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3423641199.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10240' max='25600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10240/25600 13:13 < 19:50, 12.90 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.033300</td>\n",
              "      <td>0.047743</td>\n",
              "      <td>0.990820</td>\n",
              "      <td>0.990879</td>\n",
              "      <td>0.984574</td>\n",
              "      <td>0.997266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.022221</td>\n",
              "      <td>0.995313</td>\n",
              "      <td>0.995301</td>\n",
              "      <td>0.997645</td>\n",
              "      <td>0.992969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.045030</td>\n",
              "      <td>0.994141</td>\n",
              "      <td>0.994154</td>\n",
              "      <td>0.991835</td>\n",
              "      <td>0.996484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.008100</td>\n",
              "      <td>0.033330</td>\n",
              "      <td>0.995313</td>\n",
              "      <td>0.995296</td>\n",
              "      <td>0.998820</td>\n",
              "      <td>0.991797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [640/640 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.022220568731427193, 'eval_accuracy': 0.9953125, 'eval_f1': 0.995301487862177, 'eval_precision': 0.9976452119309263, 'eval_recall': 0.99296875, 'eval_runtime': 6.1557, 'eval_samples_per_second': 831.744, 'eval_steps_per_second': 103.968, 'epoch': 4.0}\n",
            "\n",
            "========================================================\n",
            "\ud83d\udd0d Run 4/4: Training with: {'learning_rate': 1e-05, 'batch_size': 16, 'weight_decay': 0.1}\n",
            "========================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3423641199.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10240' max='12800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10240/12800 17:54 < 04:28, 9.53 it/s, Epoch 8/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.033800</td>\n",
              "      <td>0.022945</td>\n",
              "      <td>0.994141</td>\n",
              "      <td>0.994147</td>\n",
              "      <td>0.992985</td>\n",
              "      <td>0.995313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.018600</td>\n",
              "      <td>0.026893</td>\n",
              "      <td>0.994141</td>\n",
              "      <td>0.994161</td>\n",
              "      <td>0.990690</td>\n",
              "      <td>0.997656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.049853</td>\n",
              "      <td>0.992773</td>\n",
              "      <td>0.992727</td>\n",
              "      <td>0.999209</td>\n",
              "      <td>0.986328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.037623</td>\n",
              "      <td>0.994531</td>\n",
              "      <td>0.994553</td>\n",
              "      <td>0.990698</td>\n",
              "      <td>0.998437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.033258</td>\n",
              "      <td>0.994922</td>\n",
              "      <td>0.994924</td>\n",
              "      <td>0.994536</td>\n",
              "      <td>0.995313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.028010</td>\n",
              "      <td>0.996289</td>\n",
              "      <td>0.996294</td>\n",
              "      <td>0.994936</td>\n",
              "      <td>0.997656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.038234</td>\n",
              "      <td>0.995703</td>\n",
              "      <td>0.995713</td>\n",
              "      <td>0.993390</td>\n",
              "      <td>0.998047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039961</td>\n",
              "      <td>0.995313</td>\n",
              "      <td>0.995323</td>\n",
              "      <td>0.993002</td>\n",
              "      <td>0.997656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [640/640 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.028010064736008644, 'eval_accuracy': 0.9962890625, 'eval_f1': 0.9962941291203433, 'eval_precision': 0.9949357226334242, 'eval_recall': 0.99765625, 'eval_runtime': 6.306, 'eval_samples_per_second': 811.929, 'eval_steps_per_second': 101.491, 'epoch': 8.0}\n",
            "\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfc6 BEST HYPERPARAMETERS FOUND:\n",
            "{'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.05}\n",
            "Best Validation F1: 0.9971\n",
            "==================================================\n",
            "\n",
            "\ud83d\udcdd Evaluating Best Model on Held-out Test Set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [800/800 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**************************************************\n",
            "\u2b50 FINAL TEST SET RESULTS \u2b50\n",
            "Best Params: {'learning_rate': 3e-05, 'batch_size': 16, 'weight_decay': 0.05}\n",
            "Accuracy: 0.9934\n",
            "F1 Score: 0.9935\n",
            "Precision: 0.9907\n",
            "Recall: 0.9962\n",
            "**************************************************\n"
          ]
        }
      ]
    }
  ]
}