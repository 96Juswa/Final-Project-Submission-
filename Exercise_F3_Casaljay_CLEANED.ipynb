{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate optuna -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVTmU7nBCcbH",
        "outputId": "a8505f37-70a2-4303-8307-58109755514b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script performs Random Search for hyperparameter optimization\n",
        "# on a clickbait detection dataset using DistilBERT multilingual model.\n",
        "# NOTE: This process will involve multiple full training runs and may take\n",
        "# several hours to complete on Google Colab.\n",
        "\n",
        "# 1. SETUP AND INSTALLATION\n",
        "# Run this command first in your Colab notebook:\n",
        "# !pip install transformers datasets accelerate ray[tune] optuna -U\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertTokenizerFast,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    set_seed\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Set a consistent seed for reproducibility across runs\n",
        "set_seed(42)\n",
        "\n",
        "# Ensure GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, using CPU.\")\n",
        "\n",
        "# Mount Google Drive to save results\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. DATA PREPARATION ---\n",
        "\n",
        "print(\"\\n--- Loading Dataset ---\")\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the clickbait dataset\n",
        "file_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(df.head())\n",
        "\n",
        "# Check label distribution\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "print(df['clickbait'].value_counts())\n",
        "\n",
        "# Limit data size to make Grid Search feasible\n",
        "# Using 3000 training samples and 600 evaluation samples\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, eval_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['clickbait']\n",
        ")\n",
        "\n",
        "# Further limit for faster experiments\n",
        "train_df = train_df.sample(n=min(3000, len(train_df)), random_state=42)\n",
        "eval_df = eval_df.sample(n=min(600, len(eval_df)), random_state=42)\n",
        "\n",
        "print(f\"\\nTrain size: {len(train_df)}\")\n",
        "print(f\"Eval size: {len(eval_df)}\")\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df[['headline', 'clickbait']].reset_index(drop=True))\n",
        "eval_dataset = Dataset.from_pandas(eval_df[['headline', 'clickbait']].reset_index(drop=True))\n",
        "\n",
        "# Rename 'clickbait' to 'labels' for compatibility\n",
        "train_dataset = train_dataset.rename_column(\"clickbait\", \"labels\")\n",
        "eval_dataset = eval_dataset.rename_column(\"clickbait\", \"labels\")\n",
        "\n",
        "# Initialize Tokenizer\n",
        "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"headline\"], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format to PyTorch tensors\n",
        "tokenized_train.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "tokenized_eval.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "print(\"\\n--- Data Preparation Complete ---\")\n",
        "\n",
        "# --- 3. MODEL, METRICS, AND HYPERPARAMETER DEFINITION ---\n",
        "\n",
        "# Function to initialize a fresh model for each grid search run\n",
        "def model_init():\n",
        "    return DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    precision = precision_score(p.label_ids, preds, average=\"binary\", zero_division=0)\n",
        "    recall = recall_score(p.label_ids, preds, average=\"binary\", zero_division=0)\n",
        "    f1 = f1_score(p.label_ids, preds, average=\"binary\", zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "# --- HYPERPARAMETER RANDOM SEARCH DEFINITION ---\n",
        "def tune_hp(trial):\n",
        "    \"\"\"\n",
        "    This function defines the hyperparameter space for RANDOM SEARCH.\n",
        "    Random Search samples randomly from continuous and categorical distributions.\n",
        "\n",
        "    Based on your assigned hyperparameters:\n",
        "    - num_train_epochs: Random integer between 2-5\n",
        "    - per_device_train_batch_size: Random choice from [16, 32, 64]\n",
        "    - weight_decay: Random float between 0.01-0.1\n",
        "\n",
        "    This allows more flexibility than Grid Search!\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Number of training epochs (random integer)\n",
        "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
        "\n",
        "    # 2. Batch Size (random categorical)\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64])\n",
        "\n",
        "    # 3. Weight Decay (random float - continuous)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n",
        "\n",
        "    return {\n",
        "        \"num_train_epochs\": num_train_epochs,\n",
        "        \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"learning_rate\": 2e-5,  # Fixed learning rate\n",
        "    }\n",
        "\n",
        "\n",
        "# --- 4. TRAINING ARGUMENTS (Fixed for all runs) ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./random_search_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# --- 5. EXECUTION OF RANDOM SEARCH ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING RANDOM SEARCH\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nOptimizing for 'f1' score...\")\n",
        "print(\"Total random trials: 10\")\n",
        "print(\"Random Search samples randomly from the hyperparameter space.\\n\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    backend=\"optuna\",\n",
        "    hp_space=tune_hp,\n",
        "    direction=\"maximize\",\n",
        "    n_trials=10,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RANDOM SEARCH COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal time taken: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
        "\n",
        "# --- 6. RESULTS ---\n",
        "print(\"\\n--- BEST HYPERPARAMETERS FOUND ---\")\n",
        "\n",
        "if best_trial:\n",
        "    print(f\"\\nBest Trial Object: {best_trial}\")\n",
        "\n",
        "    best_hps = best_trial.hyperparameters\n",
        "    best_f1 = best_trial.objective\n",
        "\n",
        "    print(\"\\n\ud83d\udcca Best Hyperparameters:\")\n",
        "    for key, value in best_hps.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf Best F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "    # Save results to Excel\n",
        "    results_data = {\n",
        "        \"Hyperparameter\": list(best_hps.keys()),\n",
        "        \"Value\": list(best_hps.values())\n",
        "    }\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    results_df.loc[len(results_df)] = [\"Best F1 Score\", best_f1]\n",
        "    results_df.loc[len(results_df)] = [\"Total Time (minutes)\", duration/60]\n",
        "\n",
        "    excel_path = \"/content/drive/MyDrive/random_search_results.xlsx\"\n",
        "    results_df.to_excel(excel_path, index=False)\n",
        "\n",
        "    print(f\"\\n\u2705 Results saved to: {excel_path}\")\n",
        "else:\n",
        "    print(\"\u274c Search failed or no best trial found.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"To train final model with best hyperparameters:\")\n",
        "print(\"Use the best_hps dictionary shown above in a new TrainingArguments.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fefa635b0b264cae8ca38fd806ee2bad",
            "d9c655b03d854478b014f9fe3c7aaeb6",
            "b7fa283d6383465e8230081d1653a946",
            "a8c2083cb8cd4748a59f48c2d9a7e791",
            "d8c7471003584aa8a31dbc2ba17359c0",
            "3315e14e399a43398494cd4d82f53bd1",
            "93185fe298ad4ee0bdc6009e4a05aeb9",
            "c62f56a623fe40f9a3259a57753ef63e",
            "d4d77704200a41769c53ea9443a23f8a",
            "af38eac39f4446ad8543180c84290cc3",
            "4776afcf12384d178ae6e497fcd832ca",
            "a4b49fdf5de64a5ca9948e01ec10565c",
            "aac8efb235804921a1ecce5d943f49e0",
            "dffef92d430a4f6ab17708638723277d",
            "1973884b91784084b12ae5b02e142d9d",
            "2a68c9a8f41143f99260a06542e704ad",
            "63bf98f1533f4e2eacd702a6de035371",
            "229bd6435b604c20a2850ff0956f7a70",
            "b35936dda11a44c4801e0a788ecbd7b8",
            "b0eeccfa615f4447b3f9238f77fa38b3",
            "a24bff2bf25a4b82aa4ebb40288ef1e1",
            "3b345a5a62af4e1488a7a44bf1acd64e"
          ]
        },
        "id": "RIxuW_a5-0Qq",
        "outputId": "97f8d8ee-bb8b-46b9-d928-10c636d705f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "--- Loading Dataset ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5acfefd0-eefd-4066-9dfb-0dd363e97f45\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5acfefd0-eefd-4066-9dfb-0dd363e97f45\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving clickbait_data.csv to clickbait_data (2).csv\n",
            "\n",
            "Dataset shape: (32000, 2)\n",
            "Columns: ['headline', 'clickbait']\n",
            "                                            headline  clickbait\n",
            "0                                 Should I Get Bings          1\n",
            "1      Which TV Female Friend Group Do You Belong In          1\n",
            "2  The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
            "3  This Vine Of New York On \"Celebrity Big Brothe...          1\n",
            "4  A Couple Did A Stunning Photo Shoot With Their...          1\n",
            "\n",
            "Label Distribution:\n",
            "clickbait\n",
            "0    16001\n",
            "1    15999\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train size: 3000\n",
            "Eval size: 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fefa635b0b264cae8ca38fd806ee2bad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4b49fdf5de64a5ca9948e01ec10565c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data Preparation Complete ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-678106322.py:165: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-11-08 15:28:11,486] A new study created in memory with name: no-name-a199b853-544e-4340-ad23-effc10263ada\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING RANDOM SEARCH\n",
            "======================================================================\n",
            "\n",
            "Optimizing for 'f1' score...\n",
            "Total random trials: 10\n",
            "Random Search samples randomly from the hyperparameter space.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [141/141 00:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.363810</td>\n",
              "      <td>0.946667</td>\n",
              "      <td>0.924765</td>\n",
              "      <td>0.973597</td>\n",
              "      <td>0.948553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.587200</td>\n",
              "      <td>0.056655</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>0.989967</td>\n",
              "      <td>0.976898</td>\n",
              "      <td>0.983389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.113500</td>\n",
              "      <td>0.057931</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.985075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:29:14,199] Trial 0 finished with value: 3.9402726466676516 and parameters: {'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.04629549006389165}. Best is trial 0 with value: 3.9402726466676516.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [188/188 01:37, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.363797</td>\n",
              "      <td>0.946667</td>\n",
              "      <td>0.924765</td>\n",
              "      <td>0.973597</td>\n",
              "      <td>0.948553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.587200</td>\n",
              "      <td>0.056596</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>0.989967</td>\n",
              "      <td>0.976898</td>\n",
              "      <td>0.983389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.113500</td>\n",
              "      <td>0.054319</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.985075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.028200</td>\n",
              "      <td>0.054396</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.985075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:30:53,133] Trial 1 finished with value: 3.9402726466676516 and parameters: {'num_train_epochs': 4, 'per_device_train_batch_size': 64, 'weight_decay': 0.031246550560741798}. Best is trial 0 with value: 3.9402726466676516.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [564/564 01:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.060600</td>\n",
              "      <td>0.042797</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.996656</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>0.990033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>0.063405</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.996644</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.988353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.045480</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>0.986799</td>\n",
              "      <td>0.991708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:32:19,431] Trial 2 finished with value: 3.9668401392378043 and parameters: {'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.07445647733106477}. Best is trial 2 with value: 3.9668401392378043.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [188/188 01:25, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.363775</td>\n",
              "      <td>0.946667</td>\n",
              "      <td>0.924765</td>\n",
              "      <td>0.973597</td>\n",
              "      <td>0.948553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.587200</td>\n",
              "      <td>0.060890</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>0.989967</td>\n",
              "      <td>0.976898</td>\n",
              "      <td>0.983389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.113100</td>\n",
              "      <td>0.050699</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.985075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.028500</td>\n",
              "      <td>0.051614</td>\n",
              "      <td>0.986667</td>\n",
              "      <td>0.993311</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.986711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:33:46,588] Trial 3 finished with value: 3.946886686713094 and parameters: {'num_train_epochs': 4, 'per_device_train_batch_size': 64, 'weight_decay': 0.013086416480385783}. Best is trial 2 with value: 3.9668401392378043.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [282/282 01:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.592600</td>\n",
              "      <td>0.108170</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>0.946203</td>\n",
              "      <td>0.986799</td>\n",
              "      <td>0.966074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.051500</td>\n",
              "      <td>0.064138</td>\n",
              "      <td>0.986667</td>\n",
              "      <td>0.996633</td>\n",
              "      <td>0.976898</td>\n",
              "      <td>0.986667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.021100</td>\n",
              "      <td>0.041627</td>\n",
              "      <td>0.986667</td>\n",
              "      <td>0.990033</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>0.986755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:35:10,314] Trial 4 finished with value: 3.9469532059804298 and parameters: {'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.0877512281305692}. Best is trial 2 with value: 3.9668401392378043.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [141/141 01:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.363892</td>\n",
              "      <td>0.946667</td>\n",
              "      <td>0.924765</td>\n",
              "      <td>0.973597</td>\n",
              "      <td>0.948553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.587200</td>\n",
              "      <td>0.060852</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>0.989967</td>\n",
              "      <td>0.976898</td>\n",
              "      <td>0.983389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.113100</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.985075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:36:18,921] Trial 5 finished with value: 3.9402726466676516 and parameters: {'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'weight_decay': 0.030766522306489665}. Best is trial 2 with value: 3.9668401392378043.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [940/940 03:10, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.060500</td>\n",
              "      <td>0.044879</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.996644</td>\n",
              "      <td>0.980198</td>\n",
              "      <td>0.988353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>0.052758</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>0.988391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.047872</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>0.986799</td>\n",
              "      <td>0.991708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.043502</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>0.986799</td>\n",
              "      <td>0.991708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.061108</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.996656</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>0.990033</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:39:31,480] Trial 6 finished with value: 3.9601870908209946 and parameters: {'num_train_epochs': 5, 'per_device_train_batch_size': 16, 'weight_decay': 0.032521659485126087}. Best is trial 2 with value: 3.9668401392378043.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='752' max='752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [752/752 02:07, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.060600</td>\n",
              "      <td>0.044163</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.996656</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>0.990033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.056142</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.996656</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>0.990033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.041079</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.993377</td>\n",
              "      <td>0.990099</td>\n",
              "      <td>0.991736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.054677</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.996656</td>\n",
              "      <td>0.983498</td>\n",
              "      <td>0.990033</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:41:40,693] Trial 7 finished with value: 3.9601870908209946 and parameters: {'num_train_epochs': 4, 'per_device_train_batch_size': 16, 'weight_decay': 0.07934836724101123}. Best is trial 2 with value: 3.9668401392378043.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='94' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 94/376 00:07 < 00:22, 12.70 it/s, Epoch 1/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.592600</td>\n",
              "      <td>0.108165</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>0.946203</td>\n",
              "      <td>0.986799</td>\n",
              "      <td>0.966074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:41:48,773] Trial 8 pruned. \n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='94' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 94/470 00:07 < 00:28, 13.14 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.592600</td>\n",
              "      <td>0.108360</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>0.946203</td>\n",
              "      <td>0.986799</td>\n",
              "      <td>0.966074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-08 15:41:56,605] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "RANDOM SEARCH COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Total time taken: 825.20 seconds (13.75 minutes)\n",
            "\n",
            "--- BEST HYPERPARAMETERS FOUND ---\n",
            "\n",
            "Best Trial Object: BestRun(run_id='2', objective=3.9668401392378043, hyperparameters={'num_train_epochs': 3, 'per_device_train_batch_size': 16, 'weight_decay': 0.07445647733106477}, run_summary=None)\n",
            "\n",
            "\ud83d\udcca Best Hyperparameters:\n",
            "  num_train_epochs: 3\n",
            "  per_device_train_batch_size: 16\n",
            "  weight_decay: 0.07445647733106477\n",
            "\n",
            "\ud83c\udfaf Best F1 Score: 3.9668\n",
            "\n",
            "\u2705 Results saved to: /content/drive/MyDrive/random_search_results.xlsx\n",
            "\n",
            "======================================================================\n",
            "To train final model with best hyperparameters:\n",
            "Use the best_hps dictionary shown above in a new TrainingArguments.\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}